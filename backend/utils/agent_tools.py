import os
import logging
import asyncio
from typing import Any, Callable, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

logger = logging.getLogger(__name__)

async def retry_llm_call(
    llm_func: Callable,
    *args,
    max_retries: int = 1,
    retry_delay: float = 0.5,
    error_context: str = "LLMè°ƒç”¨",
    fallback_model: Optional[list[str]] = None,
    **kwargs
) -> Optional[Any]:
    """
    é€šç”¨çš„LLMè°ƒç”¨é‡è¯•åŒ…è£…å‡½æ•°ï¼Œæ”¯æŒæ¨¡å‹é™çº§

    Args:
        llm_func: LLMè°ƒç”¨å‡½æ•°ï¼ˆå¦‚ llm.ainvoke æˆ– chain.ainvokeï¼‰
        *args: ä¼ é€’ç»™llm_funcçš„ä½ç½®å‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤1æ¬¡ï¼‰
        retry_delay: é‡è¯•é—´éš”ç§’æ•°ï¼ˆé»˜è®¤1ç§’ï¼‰
        error_context: é”™è¯¯ä¸Šä¸‹æ–‡æè¿°ï¼Œç”¨äºæ—¥å¿—
        fallback_model: é™çº§æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºä¾æ¬¡å°è¯•ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨["gemini-3-flash-preview", "gpt-4o-mini"]
        **kwargs: ä¼ é€’ç»™llm_funcçš„å…³é”®å­—å‚æ•°

    Returns:
        LLMå“åº”ç»“æœï¼Œå¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åˆ™è¿”å›None
    """
    # å¦‚æœæœªæä¾›fallback_modelï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨
    if fallback_model is None:
        fallback_model = ["gemini-3-flash-preview", "gemini-2.5-flash-preview-09-2025-thinking-*"]

    consecutive_429_errors = 0  # è¿ç»­429é”™è¯¯è®¡æ•°  è²Œä¼¼æ˜¯å› ä¸ºtokenå¤ªé•¿äº†
    current_fallback_index = -1  # å½“å‰ä½¿ç”¨çš„åå¤‡æ¨¡å‹ç´¢å¼•ï¼Œ-1è¡¨ç¤ºæœªé™çº§

    for attempt in range(max_retries + 1):
        try:
            logger.debug(f"{error_context}: ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡å°è¯•")
            response = await llm_func(*args, **kwargs)

            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if response is None:
                raise ValueError("LLMè¿”å›äº†Noneå“åº”")

            logger.info(f"{error_context}: è°ƒç”¨æˆåŠŸï¼ˆå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰")
            return response

        except Exception as e:
            is_last_attempt = (attempt == max_retries)
            error_str = str(e)

            # æ£€æµ‹429é”™è¯¯ï¼ˆè´Ÿè½½é¥±å’Œï¼‰
            is_429_error = "429" in error_str or "è´Ÿè½½å·²é¥±å’Œ" in error_str or "è´Ÿè½½é¥±å’Œ" in error_str

            if is_429_error:
                consecutive_429_errors += 1
                logger.warning(f"{error_context}: æ£€æµ‹åˆ°429é”™è¯¯ï¼ˆè´Ÿè½½é¥±å’Œï¼‰ï¼Œè¿ç»­ç¬¬ {consecutive_429_errors} æ¬¡")

                # å¦‚æœè¿ç»­é‡åˆ°2æ¬¡429é”™è¯¯ä¸”è¿˜æœ‰å¯ç”¨çš„é™çº§æ¨¡å‹ï¼Œå°è¯•åˆ‡æ¢
                if consecutive_429_errors >= 2 and fallback_model and current_fallback_index < len(fallback_model) - 1:
                    current_fallback_index += 1
                    target_model = fallback_model[current_fallback_index]
                    logger.warning(f"{error_context}: è¿ç»­é‡åˆ° {consecutive_429_errors} æ¬¡429é”™è¯¯ï¼Œå°è¯•é™çº§åˆ°æ¨¡å‹ [{current_fallback_index + 1}/{len(fallback_model)}]: {target_model}")
                    try:
                        # å°è¯•è·å– config å‚æ•°å¹¶ä¿®æ”¹æ¨¡å‹
                        if 'config' in kwargs and hasattr(kwargs['config'], 'get'):
                            # å¦‚æœconfigæ˜¯å­—å…¸ç±»å‹
                            if 'configurable' not in kwargs['config']:
                                kwargs['config']['configurable'] = {}
                            kwargs['config']['configurable']['model_name'] = target_model
                        elif hasattr(llm_func, '__self__') and hasattr(llm_func.__self__, 'model_name'):
                            # å¦‚æœllm_funcæ˜¯ç»‘å®šæ–¹æ³•ï¼Œå°è¯•ä¿®æ”¹å®ä¾‹çš„model_name
                            original_model = llm_func.__self__.model_name
                            logger.info(f"{error_context}: å°†LLMå®ä¾‹çš„model_nameä» {original_model} ä¿®æ”¹ä¸º {target_model}")
                            llm_func.__self__.model_name = target_model

                        consecutive_429_errors = 0  # é‡ç½®è®¡æ•°å™¨
                        logger.info(f"{error_context}: å·²åˆ‡æ¢åˆ°é™çº§æ¨¡å‹ {target_model}ï¼Œå°†ç»§ç»­é‡è¯•")
                        continue  # ç«‹å³é‡è¯•ï¼Œä¸ç­‰å¾…
                    except Exception as degrade_error:
                        logger.error(f"{error_context}: æ¨¡å‹é™çº§å¤±è´¥: {str(degrade_error)}")
            else:
                # é429é”™è¯¯ï¼Œé‡ç½®è®¡æ•°å™¨
                consecutive_429_errors = 0

            if is_last_attempt:
                logger.error(f"{error_context}: æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼ˆ{max_retries + 1}æ¬¡å°è¯•ï¼‰")
                logger.error(f"æœ€åä¸€æ¬¡é”™è¯¯: {str(e)}")
                logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                if current_fallback_index >= 0:
                    logger.error(f"æ³¨æ„: å·²å°è¯•é™çº§åˆ°æ¨¡å‹ {fallback_model[:current_fallback_index + 1]} ä½†ä»ç„¶å¤±è´¥")
                return None
            else:
                logger.warning(f"{error_context}: ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                logger.info(f"å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
                await asyncio.sleep(retry_delay)

    return None

def get_llm():
    model = os.getenv("MODEL_NAME")
    api_key = os.getenv("MODEL_API_KEY")
    base_url = os.getenv("MODEL_BASE_URL")

    llm = ChatOpenAI(model_name=model,openai_api_key=api_key,openai_api_base=base_url,temperature=0)
    return llm

async def execute_tool_calls(ai_message, tools: list, logger_instance=None) -> list:
    """
    è‡ªå®šä¹‰å·¥å…·è°ƒç”¨å‡½æ•°ï¼Œæ›¿ä»£å†…ç½®çš„ToolNode

    Args:
        ai_message: LLMè¿”å›çš„åŒ…å«å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯
        tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        logger_instance: æ—¥å¿—è®°å½•å™¨å®ä¾‹

    Returns:
        åŒ…å«ToolMessageçš„åˆ—è¡¨
    """
    from langchain_core.messages import ToolMessage
    import json

    log = logger_instance if logger_instance else logger
    tool_messages = []

    # åˆ›å»ºå·¥å…·åç§°åˆ°å·¥å…·å¯¹è±¡çš„æ˜ å°„
    tool_map = {tool.name: tool for tool in tools}

    # å…¼å®¹æ–°æ—§ç‰ˆæœ¬çš„tool_callsæ ¼å¼
    tool_calls = []
    if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls:
        tool_calls = ai_message.tool_calls
        log.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼ˆæ–°ç‰ˆæœ¬æ ¼å¼ï¼‰ï¼Œæ•°é‡: {len(tool_calls)}")
    elif hasattr(ai_message, 'additional_kwargs') and "tool_calls" in ai_message.additional_kwargs:
        # æ—§ç‰ˆæœ¬æ ¼å¼
        raw_tool_calls = ai_message.additional_kwargs['tool_calls']
        for tc in raw_tool_calls:
            tool_calls.append({
                'name': tc.get('function', {}).get('name'),
                'args': json.loads(tc.get('function', {}).get('arguments', '{}')),
                'id': tc.get('id', '')
            })
        log.info(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼ˆæ—§ç‰ˆæœ¬æ ¼å¼ï¼‰ï¼Œæ•°é‡: {len(tool_calls)}")

    if not tool_calls:
        log.warning("æœªæ£€æµ‹åˆ°ä»»ä½•å·¥å…·è°ƒç”¨")
        return []

    # é€ä¸ªæ‰§è¡Œå·¥å…·è°ƒç”¨
    for idx, tool_call in enumerate(tool_calls, 1):
        tool_name = tool_call.get('name', 'unknown')
        tool_args = tool_call.get('args', {})
        tool_id = tool_call.get('id', '')

        log.info(f"=" * 60)
        log.info(f"ã€æ‰§è¡Œå·¥å…· {idx}/{len(tool_calls)}ã€‘")
        log.info(f"å·¥å…·åç§°: {tool_name}")
        log.info(f"å·¥å…·å‚æ•°: {json.dumps(tool_args, ensure_ascii=False, indent=2)}")
        log.info(f"=" * 60)

        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·
        if tool_name not in tool_map:
            error_msg = f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨"
            log.error(error_msg)
            tool_messages.append(
                ToolMessage(
                    content=error_msg,
                    tool_call_id=tool_id,
                    name=tool_name
                )
            )
            continue

        # æ‰§è¡Œå·¥å…·
        tool = tool_map[tool_name]
        try:
            log.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œå·¥å…·: {tool_name}")

            # è°ƒç”¨å·¥å…·ï¼ˆå§‹ç»ˆä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼‰
            result = await tool.ainvoke(tool_args)

            log.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ")
            log.info(f"å·¥å…·è¿”å›ç»“æœï¼ˆå‰500å­—ç¬¦ï¼‰: {str(result)[:500]}")

            # åˆ›å»ºToolMessage
            tool_messages.append(
                ToolMessage(
                    content=str(result),
                    tool_call_id=tool_id,
                    name=tool_name
                )
            )

        except Exception as e:
            error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {type(e).__name__}: {str(e)}"
            log.error(error_msg)
            tool_messages.append(
                ToolMessage(
                    content=error_msg,
                    tool_call_id=tool_id,
                    name=tool_name
                )
            )

    log.info(f"=" * 60)
    log.info(f"æ‰€æœ‰å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå…±æ‰§è¡Œ {len(tool_messages)} ä¸ªå·¥å…·")
    log.info(f"=" * 60)

    return tool_messages