# 旅游规划系统面试问答

## 1. 项目整体介绍

**Q: 能介绍一下你的旅游规划项目吗?**

A: 这是一个基于LangGraph和MCP协议的多Agent协同旅游规划系统。核心架构是Plan-Execute-Replan三阶段循环工作流,配合父Agent统筹调度、6个专业子Agent执行任务,实现智能化的旅游攻略生成。系统集成了高德地图、12306、航班、天气、酒店、搜索等6种MCP服务,涵盖住宿、交通、天气、景点、美食等全方位规划。最大的亮点是引入了ReAct推理模式、智能缓存机制和反馈调整机制,既保证了信息完整性又控制了成本。

---

## 2. 工作流设计

**Q: 能详细介绍一下你的工作流设计吗?**

A: 我设计了Plan-Execute-Replan三阶段循环工作流:

**Plan阶段**:LLM分析用户需求生成结构化任务规划,输出overview总体规划、actionable_tasks可执行任务(按交通/天气/酒店分组)、summary_task总结任务,以及intervention_request判断是否需要人工介入。这个阶段会智能检测信息缺失,生成需要用户补充的问题。

**Execute阶段**:父Agent协调6个子Agent执行任务。父Agent分析每个任务决定分发给哪个子Agent,子Agent执行具体的MCP工具调用(如高德地图、12306),判断任务是否完成并进行总结,最后收集所有工具调用结果。

**Replan阶段**:基于执行结果生成完整的旅游攻略,输出景点、美食、交通、天气等信息,并通过need_supplement布尔值标识是否需要补充执行,如果需要则给出supplement_tasks补充任务列表。

**闭环机制**:如果Replan检测到信息缺失,返回need_supplement=True触发补充执行循环,回到Execute阶段执行补充任务,再次Replan,直到信息完整或达到最大补充次数。

这个设计最大的优势是形成了自适应的智能闭环,确保攻略的完整性。

---

## 3. 多Agent协同架构

**Q: 你的多Agent架构是怎么设计的?父Agent和子Agent如何协作?**

A: 我构建了父Agent+6个子Agent的协同架构:

**父Agent职责**:
1. 任务分发:使用LLM分析任务内容,决定分发给哪个子Agent(交通/天气/地图/搜索/酒店/文件)
2. 结果整合:收集所有子Agent的工具调用结果
3. 工作流协调:控制整个Plan-Execute-Replan流程

任务分发时,父Agent调用LLM分析任务内容匹配可用的子Agent类型,LLM返回JSON格式的决策包含选中的Agent类型和原因。如果LLM分析失败,使用关键词匹配作为降级策略(如"火车/机票"→transport)。

**子Agent设计**:6个子Agent各自绑定专业MCP工具。我设计了MCP映射配置,按MCP服务器类型绑定工具而非硬编码工具名称,这样易于扩展新的MCP服务。

**协同流程**:Execute阶段启动→父Agent获取任务列表(按类别分组)→遍历每个任务类别→父Agent调用LLM分析任务选择子Agent→子Agent执行任务(多轮工具调用)→父Agent收集工具结果→传递给Replan阶段生成攻略。

---

## 4. ReAct推理模式

**Q: 你提到的ReAct推理模式是怎么实现的?**

A: 我实现了两层ReAct循环:

**第一层:父Agent的任务分发**
- 思考:父Agent使用LLM分析任务内容,推理应该由哪个子Agent处理,返回JSON决策
- 行动:将任务分发给选中的子Agent执行
- 观察:收集子Agent的执行结果

**第二层:子Agent的工具调用循环**
1. 思考:子Agent调用LLM分析任务决定调用哪个工具,LLM返回工具调用决策
2. 行动:根据LLM决策执行具体的工具调用(如调用12306-mcp的search_trains),获取工具返回结果
3. 观察:使用专门的Prompt让LLM批判结果是否真正满足任务要求,返回格式"0|未完成原因"或"1|完成原因"

关键创新是不是简单检查是否有工具调用,而是使用LLM批判结果是否真正满足任务要求。如果未完成会进入下一轮循环继续补充查询。

**额外机制**:
- 额外轮次机制:主循环结束后任务仍未完成,最多额外2轮,每轮都会告知LLM未完成的原因引导其精准补充
- Fallback机制:2轮额外轮次后仍未完成,调用zhipu_search作为最后的fallback,基于任务和未完成原因生成搜索关键词

这个设计大大提高了信息的完整性,从简单的工具调用升级为智能的任务完成度验证。

---

## 5. 工具集成

**Q: 你集成了哪些MCP工具?如何统一调用接口?**

A: 我集成了6种MCP服务:
- amap-maps:POI搜索、路线规划,绑定地图助手
- 12306-mcp:火车票时刻、价格查询,绑定交通助手
- variflight-mcp:航班信息查询,绑定交通助手
- mcp_tool:天气预报查询,绑定天气助手
- aigohotel-mcp:酒店搜索、价格对比,绑定酒店助手
- zhipu_search:互联网搜索,作为搜索助手的fallback

**统一调用接口**:我开发了MCPManager单例管理器,功能包括:
1. 自动连接所有配置的MCP服务器
2. 按服务器分组工具:get_tools_by_server()
3. 根据配置映射创建子Agent:create_sub_agents()

**工具调用流程**:从MCP管理器获取工具(按服务器分组)→根据配置创建子Agent并绑定工具→子Agent执行时调用工具获取结果并存储。

每次工具调用后自动保存到JSON文件,按类别分类(transport、hotel、weather等),存储工具输入、输出、上下文信息,用于缓存复用和后续RAG检索。

---

## 6. 智能缓存机制

**Q: 你是如何实现缓存机制的?如何避免重复调用?**

A: 我设计了工具结果缓存与复用策略:

**缓存存储策略**:按任务类别分别存储到JSON文件(transport.json、hotel.json、weather.json),每个工具执行结果保存为一条记录,包含时间戳、类别、工具名称、工具输入/输出、执行上下文、元数据(任务描述、Agent名称、执行轮次)。

**保存时机**:每次工具调用后立即保存到对应类别的JSON文件中。

**缓存复用策略**:支持两种匹配模式:
1. 精确匹配:tool_input必须完全相同,适用于对参数敏感的场景
2. 模糊匹配(默认):过滤空值参数(None、""),只比较非空参数是否相同

调用前基于历史执行结果进行检索,命中相同查询时直接复用,跳过实际工具调用。

**缓存优势**:
1. 降低API成本:避免重复调用相同的MCP服务
2. 减少响应延迟:直接从本地JSON文件读取(毫秒级)
3. 支持RAG检索:按上下文条件查询历史执行记录
4. 数据分析:可以分析工具使用频率和成功率

这个机制大幅降低了API成本和响应延迟。

---

## 7. 反馈调整机制

**Q: 用户反馈后如何调整计划?如何避免全量重新生成?**

A: 我实现了基于用户反馈的增量优化:

**反馈模式触发**:用户对生成的旅游攻略不满意提交反馈(如"酒店推荐太少了"、"调整餐饮预算")→系统设置is_feedback_mode=True→重新进入Plan/Replan阶段。

**增量优化策略**:
- Plan阶段:使用专用Prompt AMUSEMENT_SYSTEM_PLAN_FEEDBACK_TEMPLATE,输入用户反馈内容和原始计划摘要,只生成调整相关的任务,不重复规划已完成的部分
- Replan阶段:使用专用Prompt AMUSEMENT_SYSYRM_REPLAN_FEEDBACK_TEMPLATE,输入用户反馈内容和完整的原始amusement_info(JSON格式),只调整用户不满意的特定部分

**智能合并策略**:核心理念是保留原始信息为基础,只合并LLM返回的改进部分:
1. 空值跳过:LLM返回空值时保留原始数据
2. 复杂字段深度合并:transportation、accommodation等字段只更新有值的子字段
3. 内容丰富度比较:对于list/dict字段比较长度,保留内容更丰富的版本
4. 其他字段直接更新:用户明确提到的部分直接使用新数据

针对不满意部分重新规划,通过智能数据合并策略保留未调整部分,避免全量重新生成,提升响应效率。

**效率提升**:避免全量重新生成节省Token成本约60-70%,保留未调整部分的详细程度不会被降低,精准修改用户不满意的特定部分。

---

## 8. 智能交互机制

**Q: 系统如何判断需要人工介入?支持哪些交互方式?**

A: 我实现了人工介入判断机制:

**LLM自主判断机制**:使用PlanWithIntervention和ReplanWithIntervention格式,让LLM返回是否需要介入以及介入的具体问题、选项类型、是否允许多选等配置。LLM自主判断信息不足并暂停请求用户补充,支持多轮交互与问答历史记录。

**判断依据**:
- 用户需求不够具体(如"预算充足"但没有具体金额)
- 存在多个合理的选择(如交通方式、住宿区域)
- 需要用户偏好(如文化历史 vs 自然风光)

**支持的交互方式**:
- 文本输入:allow_text_input: true,自由输入文字
- 单选:multi_select: false + options,选择一个选项
- 多选:multi_select: true + options,选择多个选项
- 确认对话框:options: ["继续", "重新规划"],确认或取消

**人工介入流程**:Plan/Replan阶段→LLM判断need_intervention?→YES→设置intervention_stage→跳转到wait_user_plan/wait_user_replan节点→流程暂停,状态保存到数据库→返回session_id给前端→用户在前端看到问题并回答→前端调用/resume接口传入session_id和用户答案→后端更新state的intervention_response→重新调用graph.ainvoke(state)→resume_router根据intervention_stage决定恢复位置→重新执行Plan/Replan处理用户的答案→继续后续流程。

**避免过度介入**:
1. 记录历史问答:将每次介入的问题和答案保存到collected_info["asked_questions"]
2. 传递给LLM:Plan/Replan阶段可以看到所有历史问答
3. 智能避免重复:LLM能看到之前已经问过的问题,避免重复提问

支持不限次数的多轮交互,每次介入后用户的答案会被记录,下一轮Plan/Replan可以看到完整的历史问答。

---

## 9. 上下文管理

**Q: 如何控制Token消耗?消息压缩策略是什么?**

A: 我开发了消息压缩算法,保留ToolMessage+最近N条消息,在保证上下文完整性的同时控制Token消耗:

**核心策略**:
1. 分类消息:将消息分为ToolMessage(工具调用结果)和其他消息(Human/AI/System消息)
2. 保留最近消息:保留最近5-10条其他消息,其余标记为旧消息
3. LLM总结旧消息:使用LLM将旧对话历史总结为简洁概要,保留关键信息
4. 组合压缩后的消息:总结消息+所有ToolMessage+最近其他消息

**分场景压缩阈值**:
- 正常Plan阶段:max_messages=15,平衡上下文完整性和成本
- 反馈模式Plan:max_messages=5,只需关注反馈内容减少历史
- 正常Replan阶段:max_messages=15,需要工具结果但不需要太多对话历史
- 反馈模式Replan:max_messages=20,需要完整上下文理解用户意图

**降级策略**:如果LLM总结失败,使用简单截断策略:只保留ToolMessage和最近的其他消息,丢弃旧消息。

**压缩效果**:原始50条消息→压缩后36条消息,Token节省约40%(原始约15,000 tokens→压缩后约9,000 tokens)。

**保证上下文完整性的三层保障**:
1. ToolMessage完整保留:工具调用结果最重要不能丢失
2. 最近消息完整保留:最新的5-10条对话保持完整
3. 旧消息被总结:关键信息被LLM提炼成概要

优势是大幅降低Token成本(30-40%),保留最重要的工具结果,上下文连续性不受影响,支持超长对话(100+轮)。

---

## 10. 短期与长期上下文管理

**Q: 你的项目如何管理上下文?如何处理长对话?**

A: 我设计了完善的短期和长期上下文管理策略:

**短期上下文**:当前对话中最近的消息和状态,直接参与LLM推理的上下文。包含最近N条消息(5-15条)、当前任务的工具调用结果(所有ToolMessage)、用户当前的需求和反馈、Plan/Execute/Replan阶段的输入输出。特点是容量有限(受模型上下文窗口限制)、实时性强直接影响当前任务决策、随对话进行而动态变化、每次LLM调用都会发送Token成本高。

**长期上下文**:历史对话、知识库、缓存数据等持久化存储的信息,需要时可以检索和利用。包含工具执行结果缓存(JSON文件存储的transport.json、hotel.json、weather.json等)、历史问答记录(collected_info["asked_questions"])、用户反馈历史(用于智能合并的原始攻略完整信息)、消息压缩后的总结(将旧对话通过LLM总结为概要)。特点是容量可以很大(文件系统、数据库)、持久化存储不会丢失、按需检索加载到短期上下文、存储成本低检索速度快。

**短长期上下文的转换机制**:
1. 消息压缩算法(短期→长期):原始短期上下文(50条消息)→分类处理→ToolMessage保留在短期上下文(最重要)、最近消息保留在短期上下文(当前任务相关)、旧消息LLM总结转为长期上下文(压缩总结)→新的短期上下文(36条消息,节省30-40% Token)
2. 缓存机制(长期→短期):第一次查询调用MCP工具→保存到长期上下文(transport.json)→第二次查询相同路线→从长期上下文检索(毫秒级零成本)→加载到短期上下文使用
3. 反馈调整(短长期协作):长期上下文保存原始完整攻略(持久化存储详细信息)、短期上下文承载用户反馈+LLM调整、智能合并保留长期上下文的详细信息融入短期上下文的调整

**设计价值**:
1. 成本优化:短期上下文每个请求都要发送给LLM Token成本高,长期上下文存储成本低需要时才加载。项目效果是消息压缩节省30-40% Token,缓存避免重复MCP调用。
2. 上下文窗口限制:模型上下文窗口有限,长期上下文可以无限扩展,只把需要的部分加载到短期上下文。
3. 性能优化:短期上下文实时推理速度快,长期上下文检索可以并行不影响推理速度。缓存命中直接返回(毫秒级),MCP调用工具执行(秒级)。

---

## 11. 项目亮点总结

**Q: 这个项目最大的亮点是什么?**

A: 我认为最大的亮点有三个:

**1. ReAct推理模式的任务完成度验证**:传统的Agent调用工具后就认为任务完成,但我设计了任务完成度检查机制,使用LLM批判结果是否真正满足需求,如果未完成会自动进入下一轮补充查询,这大大提高了信息的完整性。

**2. 智能反馈调整的增量优化**:很多系统在用户反馈后会全量重新生成,成本高且可能丢失原有信息。我设计了智能合并策略,基于内容丰富度比较,只更新用户不满意的特定部分,节省Token成本60-70%。

**3. 完善的上下文管理策略**:通过短期和长期上下文的分层管理,结合消息压缩算法(节省30-40% Token)和智能缓存机制(避免重复MCP调用),既保证了信息完整性又大幅降低了成本,支持超长对话。

这个项目不是简单的工具调用堆砌,而是一个完整的闭环系统:Plan阶段识别缺失→Execute阶段补充→Replan阶段再次检测→继续补充;用户反馈→增量调整→智能合并→保留优化;人工介入→暂停流程→用户响应→恢复执行。每个环节都有检测、反馈、调整机制,形成了一个自适应的智能系统。

---

## 12. 项目挑战与解决方案

**Q: 项目中遇到的最大挑战是什么?如何解决的?**

A: 最大的挑战是如何平衡成本和质量。多Agent协同、ReAct循环、工具调用都会产生大量的API成本,但如果限制太严又会影响攻略的完整性和准确性。

**我通过三个策略解决**:
1. 设计了智能缓存机制:避免重复调用MCP服务,降低API成本和响应延迟
2. 实现了消息压缩算法:降低Token消耗30-40%,保留关键信息不丢失
3. 为不同子Agent设置不同的最大轮次:交通/天气助手1轮,其他2轮,根据任务复杂度灵活控制

此外还有反馈调整的增量优化(节省60-70% Token)、分场景压缩阈值(正常/反馈模式使用不同的max_messages)等策略,最终实现了成本和质量的良好平衡。

---

## 13. 可扩展性设计

**Q: 如果需要添加新的MCP服务或新的子Agent,如何扩展?**

A: 系统设计了很好的可扩展性:

**添加新的MCP服务**:
1. 在config/mcp.py中添加新的MCP服务器配置(URL、传输方式、认证信息)
2. 在MCP_TO_AGENT_MAPPING中配置MCP服务器到Agent类型的映射
3. MCPManager会自动连接新的MCP服务器并按配置分组工具
4. 无需修改业务逻辑代码,即可在对应的子Agent中使用新工具

**添加新的子Agent**:
1. 在sub_agents.py中定义新的子Agent类,继承基础Agent类
2. 配置System Prompt描述该Agent的职责和能力
3. 在MCP_TO_AGENT_MAPPING中添加映射关系
4. 父Agent的任务分发逻辑会自动识别新的Agent类型

关键是我设计了MCP映射配置,按MCP服务器类型绑定工具而非硬编码工具名称,以及父Agent的LLM驱动的任务分发机制,使得系统可以灵活扩展新的服务和Agent,而不需要大量修改现有代码。

---

## 14. 技术栈与工具

**Q: 项目使用了哪些技术栈?**

A:
- 核心框架:LangGraph(工作流编排)、LangChain(Agent框架)
- 协议:MCP(Model Context Protocol,工具集成协议)
- LLM:支持多种大模型(通过LangChain抽象)
- 工具集成:高德地图、12306、航班、天气、酒店、搜索等6种MCP服务
- 数据存储:JSON文件缓存、数据库(状态持久化)
- 前后端交互:RESTful API(/resume接口支持人工介入)

项目的技术选型考虑了可扩展性、成本控制和开发效率,LangGraph提供了强大的状态管理和流程编排能力,MCP协议统一了工具调用接口,使得集成新服务变得非常简单。
